========================DOCKERS, KUBERNETES & JENKINS==============================


Introduction & Expected output:

--> As of now in backend we consist of 39 microservices and 5 common dependencies. Expected output is for each microservices we have to build docker image and put that image in kubernetes cluster. Also we have to do jenkins configuration for auto trigger & notification of each services during the build (That is: In backend side after the code development developer push the latest code to gitlab at the time of push event the latest code will update in docker image and kubernetes automatically) 

--> Document overview

1. Docker basics, kubernetes basics, Docker & kubernetes in single master and worker node.
2. Cluster setup for multiple nodes (Total setup and Testing in kubenetes).
3. Jenkins configuration and testing

=======================================================================================
 
Index:
(Note: For each topics below i provided link. check it for further clarification.)

1. Docker basics, kubernetes basics, Docker & kubernetes in single master and worker node.

1.1.    Docker basics.
1.2.1.  Edit project.
1.2.2.  Docker image creation.
1.3.    Kubernetes cluster setup for 1 master & 1 worker node.
1.4.    Kubernetes dashboard deploymet.
1.5.    Metric-server deployment.
1.6.    Tar file creation and push.
1.7.    Private registry creation.
1.8.    Deployment in master-node.
1.9.    Horizontal-pod autoscaleing.
1.10.   Service creation.
1.11.   Cert-manager.
1.12.   Deploy and use Nginx ingress controller.
1.13.   Setup MetalLB Load Balancing for Bare Metal Kubernetes.

2. Cluster setup for multiple nodes (Total setup and Testing in kubenetes)

2.1.	Kubernetes cluster setup for multiple master & worker nodes.
2.2.	kuberneets pod and node testing.
2.3.    Load Balancer, metallb, ingress & metrics server. 
2.4.    Testing the kubernetes  svc, ingress.

3. Jenkins configuration and testing

3.1.	 jenkins setup.
3.2.	 script creation for each services.
3.3.	 Ci/cd pipeline (gitlab & jenkins configuration).
=================================================================================

1. Docker basics, kubernetes basics, Docker & kubernetes in single master and worker node.


1.Dockerbasics
--------------

docker basics: total 15 vedios 

https://www.youtube.com/playlist?list=PLhW3qG5bs-L99pQsZ74f-LC-tOEsBp2rK

-----------------------------------------------------------------
-------------------------------------------------------------------




2.1.Edit project
----------------

. Git-clone our microsrvices from gitlab
Eg: signin in to gitlab as zm.team3 or zm.team4 --> inside select the required microservice check the latest update of that services and copy clone.
$ git clone -b <latest update eg:s3_Release> <paste the https clone you copied> 

. Imported the project in Ide (eclipse (or) sts)
Eg: import the Existing Maven projects we git cloned at eclipe

. Inside the project create a Dockerfile 
Eg: inside dockerfile 
FROM openjdk:8-alpine
EXPOSE 8080
ADD target/livfit-registration-service.jar livfit-registration-service.jar
ENTRYPOINT ["java","-jar","/livfit-registration-service.jar"]

. Open pom.xml add finalName tag
Eg:inside <build> tag add <finalName>livfit-registration-service</finalName>

. Update and run the project
For more references check the link below

https://www.youtube.com/watch?v=e3YERpG2rMs&ab_channel=JavaTechie
-----------------------------------------------------------------
------------------------------------------------------------------




2.2.Docker image creation
-------------------------

IN terminal:
. Go to the terminal in location where our project imported
Eg:
cd backend
cd livfit-registartion-service

. image build
--> docker build -t (name specified in pom.xml) .
--> docker run -p 8200:8200  livfit-registration-service.jar .
note: port no should be same. check with application property of the project

. image will be created

. check images by 
--> docker images -a

.check container by 
--> docker ps -a

For more references check the link below

https://www.youtube.com/watch?v=e3YERpG2rMs&ab_channel=JavaTechie

-----------------------------------------------------------------
------------------------------------------------------------------




3.Kubernetes setup
------------------

server 1 : 164.52.197.123
server 2 : 164.52.197.134

kubernetes setup : 
164.52.197.123 --> master
164.52.197.134 --> worker

in master:
--> :~$ sudo vi /etc/hosts
164.52.197.123 master
164.52.197.134 worker

in worker:
--> :~$ sudo vi /etc/hosts
164.52.197.123 master
164.52.197.134 worker

To check the connection ping worker from masterip
in master:
$ ping worker
PING worker (164.52.197.123) 56(84) bytes of data.
64 bytes worker (164.52.197.123): icmp_seq=1 ttl=64 time=0.462 ms
64 bytes worker (164.52.197.123): icmp_seq=2 ttl=64 time=0.686 ms

To check the connection ping master from workerip
in worker:
$ ping master
PING master(164.52.197.134) 56(84) bytes of data.
64 bytes from master (164.52.197.134): icmp_seq=1 ttl=64 time=0.238 ms
64 bytes from master (164.52.197.134): icmp_seq=2 ttl=64 time=0.510 ms

--> Install Docker on both master and worker node
$ sudo apt-get update
$ sudo apt install docker.io

--> Disable the firewall and turnoff the both master and worker node
~$ sudo ufw disable
Firewall stopped and disabled on system startulxc exec kmaster bashp

--> Install package on both master and worker node
~$ sudo apt-get update && sudo apt-get install -y apt-transport-https

--> Download the public keys
~$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
OK

--> Add kubernetes repo on both master and worker node
~$ sudo bash -c 'echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list'

--> Install kubernetes on both master and worker node
$ sudo apt-get update && sudo apt-get install -y kubelet kubeadm kubectl

--> Enable and Start kubelet on both master and worker node
$ sudo systemctl enable kubelet
$ sudo systemctl start kubelet

--> Initialize the kubernetes cluster in master node
$ sudo kubeadm init --apiserver-advertise-address=164.52.197.123 --pod-network-cidr=10.244.0.0/16
if error: sudo swapoff -a
then init the kubernetes cluster

--> Move kube config file to current user (only run on master)
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

-->Apply CNI from kube-flannel.yml(only run on master)
$ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
$ ip a s
$ vi kube-flannel.yml
- --iface=eth1
       args:
       - --ip-masq
       - --kube-subnet-mgr
       - --iface=eth1

~$ kubectl apply -f kube-flannel.yml
Eg: podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created

-->Join worker nodes to master(only run on worker)
before that add sudo to the code (copy the token from  master init)
type sudo swapoff -a
eg:sudo kubeadm join 164.52.197.123:6443 --token 99cf77.6jotojrez3rd8ijf \
   --discovery-token-ca-cert-hash sha256:b95d9ef12d6208b4c4205a3a177247f21d2bb130c57c5c59622c4328e5e9d6d9

check docker and kubelet status from two nodes
-->systemctl status docker
-->systemctl status kubelet

-->Check the nodes status(only run on master)
$ kubectl get nodes
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   26m   v1.18.2
worker   Ready    <none>   63s   v1.18.2
-------------------------------------------------

Tips:
If worker or master node not in ready status:
-1 :
check kubelet and docker are in active
$ systemctl status docker
$ systemctl status kubelet
if not restart the docker and kubelet
$ systemctl restart docker
$ systemctl restart kubelet

-2:
$ swapoff -a (put in both master and worker) and check

-3: till not solved
$ packages and public key are not correctly deployed check with that.
-----------------------------------------------------------------

For more references check the link below
https://jhooq.com/14-steps-to-install-kubernetes-on-ubuntu-18-04-and-16-04/
(ignore vagrant and follow the step for kubernetes setup)
-----------------------------------------------------------------
--> kubectl get pods --all-namespaces
all pod should be in running status 

----------------------------------------------------------------------------
----------------------------------------------------------------------------




4.kubernetes-dashboard deployment
---------------------------------
Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources


--> git clone https://github.com/justmeandopensource/kubernetes
--> cd kubernetes 
--> cd dashboard

https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
--> kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml
check pod is running by
--> kubectl get pod -n kubernetes-dashboard
https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
two methods:
1.port forwarding
2.node port

We use node port

-->kubectl -n kubernets-dashboard edit svc kubernetes-dasboard
change the type:clusterip to type:NodePort

create service account:
--> kubectl create -f sa_cluster_admin.yaml
--> kubectl -n kube-system get sa
--> kubectl -n kube-system describe sa dashboard-admin
copy the token
Eg:   Tokens:  dashboard-admin-token-l5r7n
--> kubectl -n kube-system describe secret (paste the token)
copy the generated token

By putting <masterip>:<service port>
--> kubectl get svc -n kubernetes-dashboard (cmd to get service port)
Eg: http://164.52.197.123:31435/ (it shows "Client sent an HTTP request to an HTTPS server") by adding https problem solved
Eg: https://164.52.197.123:31435
if not working in chrome try with firefox browser it will work.

Then copy the generated token.
For more references check the link below
https://www.youtube.com/watch?v=6MnsSvChl1E&ab_channel=JustmeandOpensource
-----------------------------------------------------------------
-----------------------------------------------------------------




5.metrics-server deployment
---------------------------
The Kubernetes Metrics Server is an aggregator of resource usage data in your cluster.
In masterip:
-->wget  https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.7/components.yaml
-->ls
--> mv component.yaml metrics-server.yaml
--> kubectl apply -f metrics-server.yaml

Check by this cmd to ensure metrics-server is working
--> kubectl top nodes 
--> kubectl top pods -n kube-system

output for kubectl top nodes : 
root@master-node:~# kubectl top nodes
NAME          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
master-node   247m         6%     6442Mi          55%       
worker01      114m         2%     5122Mi          44% 

If error arrises:
check the error by this cmd
--> kubectl -n kube-system logs -f <metrics server pod name>
if error is about "cannot validate certificate" or "unable to fully scrape metrics from source" then follow the steps
--> vi metrics-server.yaml
add two argument inside kind:Deployment
	args:
         - --kubelet-preferred-address-types=InternalIP
         - --kubelet-insecure-tls
save & exit and check with kubectl top nodes it will works fine.
For more references check the link below

https://www.youtube.com/watch?v=PEs2ccoZ3ow&app=desktop&ab_channel=JustmeandOpensource

-----------------------------------------------------------------

.At present docker image is created and kubernetes is set up successfully
.we have to copy the docker image in to root@164.52.197.134 and to create private repositary in server. Push the image and deploy in the master node
-----------------------------------------------------------------
-----------------------------------------------------------------




6.Tar file creation and push
----------------------------
To copy the image from our pc to server (164.52.197.134) we have to create tar file for an docker image.

1.To create tar.file:
--> step1: docker images -a (select the image)
--> step2: docker run (image you selected)
--> step3: docker ps -a (as this shows all the conatiner)
--> step4: copy the container id for the image
--> step5: docker commit (container id) (new image name)
using docker commit it will create new docker image from that container.
--> step6: docker save (new image: version)  >  (tar file)
for eg if we worked in cd Desktop -->tar file will save in desktop folder.

2.Share tar.file:
open terminal go to the location where tar.file is available.
--> scp (tar file name) root@164.52.197.134:/root

3.in 164.52.197.134
go to root and give $ls and check tar.file is received
--> docker load < (tar file name)
now check --> docker images -a
docker image created in our pc is available now in server (164.52.197.134)
---------------------------------------------------------------------------
---------------------------------------------------------------------------



7.Private registry push
-----------------------
open 164.52.197.134 server:

--> git clone https://github.com/justmeandopensource/docker
--> cd docker
--> cd docker-compose
--> cd docker registry
--->ls
-->mv oi-plain-http.yaml docker-compose.yaml
-->docker-compose up -d
-->docker-compose ps
docker build -t localhost:5000/(docker image : version) .
docker push localhost:5000/(docker image : version)

after create certification and login authentication by following the link i mentioned below
https://www.youtube.com/watch?v=r15S2tBevoE&app=desktop&ab_channel=JustmeandOpensource
------------------------------------------------------------------------
------------------------------------------------------------------------




8.Deployment and service creation 
---------------------------------
(Note: since docker private repository consist of user & password we supposed to create secret before deployment. If repository didn't consist of any credentials no need to create secret before deployment) 

in master server create secrets with authenticatiion details you created and deploy our image using yml file.

-->kubectl create secret docker-registry myregistry --docker-server=164.52.197.134:5000 --docker-username=zmast --docker-password=zmast@123 
username and password is what i created in our private registry authentication. 

-->gedit prob.yml
Eg:prob-yml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: backend-test
 labels:
   app: backend
spec:
 selector:
   matchLabels:
     app: backend
 template:
   metadata:
     labels:
       app: backend
   spec:
     containers:
     - name: backend
       image: 164.52.197.134:5000/zmast/production-registration:v1
       imagePullPolicy: Always
       ports:
       - containerPort: 8200
     imagePullSecrets:
     - name: myregistry
check it by --> cat prob.yml

--> kubectl apply -f prod.yml
check the deployment is in running states
--> kubectl get all
shows error --> check the error by following cmds
--> kubectl get events -w (or) kubectl describe pod (pod name)
-----------------------------------------------------------------
docker image created successfull
kubernets setup successfull
dashboard and metrics-server successfull
image copied to one server --> push to private repositary and pull it from our master-node successfull
-----------------------------------------------------------------
-----------------------------------------------------------------




9.Horizontal-pod creation
-------------------------
in master-node(164.52.197.123) :

horizontal pod autoscaleing for deployment we created in master-node

check the cmd for metrics-server is working
--> kubectl top nodes
--> kubectl top pods -n kube-system
if output not shows error move to next or else again go to <<<<5.metrics-server deployment>>>> and check everything is fine.

To deploy hpa:
--> kubectl edit deploy (deployment-name)
add:

resources:
	limits:
		cpu: "100m"
	requests:
		cpu: "100m"

--> kubectl autoscale deploy (deployment name) --min 1 --max 5 --cpu-percent 20
--> kubectl describe hpa (deployment name)
For more references check the link below
https://www.youtube.com/watch?v=uxuyPru3_Lc&app=desktop&ab_channel=JustmeandOpensource 
-------------------------------------------------------------------------
-------------------------------------------------------------------------



10.Service creation
-------------------
in master-node(164.52.197.123) :

--> kubectl expose deployment (deployment-name) --type=NodePort
Eg: kubectl expose deployment backend-reg-req --type=NodePort

--> kubectl get svc
Eg: NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
backend-loggedin   NodePort    10.100.221.104   <none>        9100:31015/TCP   26h
backend-reg-req    NodePort    10.109.19.77     <none>        8210:31476/TCP   27h
backend-test       NodePort    10.104.82.55     <none>        8200:30741/TCP   2d6h
kubernetes         ClusterIP   10.96.0.1        <none>        443/TCP          8d
-----------------------------------------------------------------
To check in postman 164.52.197.123:(service) and give the call
Eg: http://164.52.197.123:30741/register/get
-----------------------------------------------------------------
-----------------------------------------------------------------




11.Cert Manager
---------------
in master node(164.52.197.123):

-----<note:https://cert-manager.io/v0.12-docs/installation/kubernetes/>------
follow all the step in the above link 

but trying to apply this cmd
-->helm install \
--name cert-manager \
--namespace cert-manager \
--version v0.12.0 \
 jetstack/cert-manager

shows error in --name tag
as this error is shown because cmd used for helm before version

so instead of this cmd we used

-->helm template cert-manager jetstack/cert-manager --namespace cert-manager | kubectl apply -f -

and followed the remaining steps everything is fine.
-----------------------------------------------------------------
-----------------------------------------------------------------




12.Deploy and use Nginx ingress controller 
------------------------------------------
--> helm install my-nginx stable/nginx-ingress
check with "kubectl get all" if svc LoadBalancer is pending status fellow the "13.Setup MetalLB Load Balancing for Bare Metal Kubernetes" and continue error will resolve
--> cd yamls
--> cd ingress
--> cd ingress-demo
after create deployment , expose the deployment to create service and create ingress yaml file to configure path for each port.
for more clarification refer below link

https://www.youtube.com/watch?v=2VUQ4WjLxDg&ab_channel=JustmeandOpensource

what i have done:
installed ->  helm install with namespace example
created deployment,service and ingress with namespace example

have to create ingress for each services

Eg: registerRequest service
  Host                      Path  Backends
  ----                      ----  --------
  livfit04.providerdom.com  
                            /registerRequest   backend-reg-req:80 (10.244.1.33:8210)
  Annotations:                kubernetes.io/ingress.class: nginx
			      nginx.ingress.kubernetes.io/rewrite-target: /registerRequest

Eg: loggedin service
  Host                      Path  Backends
  ----                      ----  --------
  livfit04.providerdom.com  
                            /user   backend-loggedin:80 (10.244.1.23:9100)
  Annotations:                kubernetes.io/ingress.class: nginx
                              nginx.ingress.kubernetes.io/rewrite-target: /user
-----------------------------------------
livfit04.providerdom.com/registerRequest/applicationRoles ----> working
livfit04.providerdom.com/user/get ---> working
-----------------------------------------------------------------
-----------------------------------------------------------------




13.Setup MetalLB Load Balancing for Bare Metal Kubernetes
---------------------------------------------------------
for Eg:
--> kubectl run nginx --image nginx
--> kubectl expose deploy nginx --port 80 --type LoadBalancer
if we check the svc "kubectl get svc" the external ip of LoadBalancer is in <pending> the solution is metallb


https://metallb.universe.tf/


-go to installation
follow the installation by manifest
-go to Layer 2 configuration

--> cat <<EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 164.52.197.123 - 164.52.197.150
-->EOF

after try this 
--> kubectl run nginx --image nginx
--> kubectl expose deploy nginx --port 80 --type LoadBalancer
now external IP is shown 

more clarification check the link below

https://www.youtube.com/watch?v=xYiYIjlAgHY&ab_channel=JustmeandOpensource

=================================================================================
======******************************************************************=========
=================================================================================



2. Cluster setup for multiple nodes (Total setup and Testing in kubenetes)


2.1.	Kubernetes cluster setup for multiple master & worker nodes.
--------------------------------------------------------------------

Cluster setup for multiple master & worker:

master 1  -->  IP : 164.52.203.108 / 172.16.122.21 --> e2e-71-108 
master 2  -->  IP : 164.52.202.169 / 172.16.120.77 --> e2e-70-169
loadbalancer -->  IP : 164.52.203.102 / 172.16.121.251
worker 1 -->  IP : 164.52.202.246 / 172.16.121.148 --> e2e-70-246
worker 2 -->  IP : 164.52.203.104 / 172.16.122.7   --> e2e-71-104
worker 3 -->  IP : 164.52.203.168 / 172.16.121.136 --> e2e-71-108

========================
LoadBalancer :
--------------

> Switch as root - sudo -i
++++++

# Update your repository and your system
> sudo apt-get update && sudo apt-get upgrade -y
++++++

# Install haproxy
> sudo apt-get install haproxy -y
++++++

# Edit haproxy configuration
> Append the below lines to vi /etc/haproxy/haproxy.cfg

frontend kubernetes-frontend
    bind "LOADBALANCER PRIVATE IP":6443
    mode tcp
    option tcplog
    default_backend kubernetes-backend

backend kubernetes-backend
    mode tcp
    option tcp-check
    balance roundrobin
    server kmaster1 "MASTER1 PRIVATE IP":6443 check fall 3 rise 2
    server kmaster2 "MASTER2 PRIVATE IP":6443 check fall 3 rise 2
++++++

# Restart haproxy service
> systemctl restart haproxy	
> systemctl status haproxy
++++++

=======================
On all kubernetes nodes (Master 1, Master 2 , worker 1, worker2)
----------------------------------------------------------------

# Disable Firewall
> ufw disable
++++++

# Disable swap
> swapoff -a; sed -i '/swap/d' /etc/fstab
++++++

# Update sysctl settings for Kubernetes networking
> 
cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
++++++

# Install kubeadm and kubelet
> apt-get update && apt-get install -y apt-transport-https curl
> curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
> 
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
> apt-get update
> apt-get install -y kubelet kubeadm
> apt-mark hold kubelet kubeadm 
++++++

# Install container runtime - docker
> sudo apt-get update
> 
sudo apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common
> curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
> 
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
> sudo apt-get update
> sudo apt-get install docker-ce docker-ce-cli containerd.io -y
++++++

======================
On any one of the Kubernetes master node (Eg: kmaster1)

# Log in to master1
# Switch to root account - sudo -i
# Execute the below command to initialize the cluster -

> kubeadm init --control-plane-endpoint=""LOADBALANCER PRIVATE IP":6443" --upload-certs --apiserver-advertise-address="MASTER1 PRIVATE IP" --pod-network-cidr=192.168.0.0/16

Eg: kubeadm init --control-plane-endpoint="172.16.121.251:6443" --upload-certs --apiserver-advertise-address=172.16.122.21 --pod-network-cidr=192.168.0.0/16

for the above cmd you will get a join cmds so make a backup because we use that join cmd to join other master and worker.

eg output you get for above cmd:
____________________________________________________________________
1.Setup kubeconfig using -

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

2.Setup new control plane (master) using
  
kubeadm join loadbalancer:6443 --token cnslau.kd5fjt96jeuzymzb \
    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 \
    --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146

3.Join worker node using

kubeadm join loadbalancer:6443 --token cnslau.kd5fjt96jeuzymzb \
    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 

NOTE

Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and don't copy and paste from here.
______________________________________________________________________
++++++
=======================
# Switch to root - sudo -i
# Execute the following cmd in master1

>  mkdir -p $HOME/.kube
>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
>  sudo chown $(id -u):$(id -g) $HOME/.kube/config
++++++
=======================

=======================
# Log in to master2
# Switch to root - sudo -i
# Check the command provided by the output of master1

# Execute the kubeadm join command for control plane on master2

kubeadm join 172.16.121.251:6443 --token v8vu5n.9mpm35ots3x7e07g     --discovery-token-ca-cert-hash sha256:67a98748d4aecf8f8399bfd43c34407d7c8d3256d808435473890acfe9d65084     --control-plane --certificate-key afd5bc5455c14b7a3733f96b1a333d6b6927cdbad7a61023de55084c15ac6a7f --apiserver-advertise-address "MASTER2 PRIVATE IP"

Eg: kubeadm join 172.16.121.251:6443 --token v8vu5n.9mpm35ots3x7e07g     --discovery-token-ca-cert-hash sha256:67a98748d4aecf8f8399bfd43c34407d7c8d3256d808435473890acfe9d65084     --control-plane --certificate-key afd5bc5455c14b7a3733f96b1a333d6b6927cdbad7a61023de55084c15ac6a7f --apiserver-advertise-address 172.16.120.77
++++++
========================

========================
# Log in to worker1 and worker2
# Switch to root on both the machines - sudo -i
# Check the output given by the init command on master1 to join worker node -

> kubeadm join loadbalancer:6443 --token cnslau.kd5fjt96jeuzymzb \
    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 

# Execute the above command on all the worker nodes -
=========================

Configure kubeconfig on loadbalancer node:
------------------------------------------

# Log in to loadbalancer node
# Switch to root - sudo -i

# Create a directory - .kube at $HOME of root
> mkdir -p $HOME/.kube
++++++

# SCP configuration file from any one master node to loadbalancer node
> scp master1:/etc/kubernetes/admin.conf $HOME/.kube/config
eg: scp e2e-71-108:/etc/kubernetes/admin.conf $HOME/.kube/config
++++++

> chown $(id -u):$(id -g) $HOME/.kube/config

# Install kubectl binary
> snap install kubectl --classic
++++++

# verfify cluster
> kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
e2e-70-169   Not Ready    master   19h     v1.19.3
e2e-70-246   Not Ready    <none>   18h     v1.19.3
e2e-71-104   Not Ready    <none>   19h     v1.19.3
e2e-71-108   Not Ready    master   19h     v1.19.3
e2e-71-168   Not Ready    <none>   3h36m   v1.19.3
===============================

Install CNI and complete installation:
-------------------------------------

# From the loadbalancer node execute -
> kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml
++++++

# This installs CNI component to your cluster.
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------


2.2.	kuberneets pod and node testing.
---------------------------------------

>kubectl get nodes 
NAME         STATUS   ROLES    AGE     VERSION
e2e-70-169   Ready    master   19h     v1.19.3
e2e-70-246   Ready    <none>   18h     v1.19.3
e2e-71-104   Ready    <none>   19h     v1.19.3
e2e-71-108   Ready    master   19h     v1.19.3
e2e-71-168   Ready    <none>   3h36m   v1.19.3
++++++

>kubectl get pods -n kube-system
NAME                                      READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6fc6f785f-rbhs7   1/1     Running   0          18h
calico-node-2fccc                         1/1     Running   0          18h
calico-node-94qj8                         1/1     Running   0          18h
calico-node-kw4bw                         1/1     Running   0          18h
calico-node-mk57h                         1/1     Running   0          3h39m
calico-node-qx789                         1/1     Running   0          18h
coredns-f9fd979d6-8b4jm                   1/1     Running   0          19h
coredns-f9fd979d6-8n6p8                   1/1     Running   0          19h
etcd-e2e-70-169                           1/1     Running   0          19h
etcd-e2e-71-108                           1/1     Running   0          19h
kube-apiserver-e2e-70-169                 1/1     Running   0          19h
kube-apiserver-e2e-71-108                 1/1     Running   0          19h
kube-controller-manager-e2e-70-169        1/1     Running   0          19h
kube-controller-manager-e2e-71-108        1/1     Running   1          19h
kube-proxy-46xzc                          1/1     Running   0          19h
kube-proxy-ds6jc                          1/1     Running   0          19h
kube-proxy-kqtpp                          1/1     Running   0          3h39m
kube-proxy-pk7xf                          1/1     Running   0          19h
kube-proxy-whgvn                          1/1     Running   0          18h
kube-scheduler-e2e-70-169                 1/1     Running   0          19h
kube-scheduler-e2e-71-108                 1/1     Running   1          19h
++++++

Note: check all nodes are in ready status also pods all are in running status suppose for calico status is running but ready shows 0/1 means there is some problem in calico installation. 
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------



2.3.    Load Balancer, metallb, ingress & metrics server. 
---------------------------------------------------------

After node and pods all are in healthy we proceed next

--> Load Balancer (load balancer is one type of services for more clarification refer kuberenetes services. ) 
--> After deployment we expose the deployment to create a services where if loadbalancer in pending hav to do metallb
Refer 1.13 where references link and metallb configuration explained.
REfer 1.5 for metrics server referance.
After this
--> kubectl get all
nginx-ingress-controller
nginx-ingress-default-backend 
as above two will available in all the svc, pod, deployment.
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------



2.4.    Testing the kubernetes  svc, ingress.
---------------------------------------------
We going to do basic deployment to test all the service and ingress are working.

Basic deployment concept/ final output:
--> In browser Firefox / chrome-incognito tab

http://livfit05.providerdom.com --> it should show nginx welcome page
http://livfit05.providerdom.com/green --> it shows green text
http://livfit05.providerdom.com/blue --> it show blue text

in kubernetes master create 3 yaml file
Eg: test1-nginx.yaml test1-green.yaml test1-blue.yaml

put the below code in each respective yaml file

root@e2e-71-102:~# cat test1-nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: nginx
  name: nginx-deploy-main
spec:
  replicas: 1
  selector:
    matchLabels:
      run: nginx-main
  template:
    metadata:
      labels:
        run: nginx-main
    spec:
      containers:
      - image: nginx
        name: nginx

root@e2e-71-102:~# cat test1-green.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: nginx
  name: nginx-deploy-green
spec:
  replicas: 1
  selector:
    matchLabels:
      run: nginx-green
  template:
    metadata:
      labels:
        run: nginx-green
    spec:
      volumes:
      - name: webdata
        emptyDir: {}
      initContainers:
      - name: web-content
        image: busybox
        volumeMounts:
        - name: webdata
          mountPath: "/webdata"
        command: ["/bin/sh", "-c", 'echo "<h1>I am <font color=green>GREEN</font></h1>" > /webdata/index.html']
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: webdata
          mountPath: "/usr/share/nginx/html"

root@e2e-71-102:~# cat test1-blue.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: nginx
  name: nginx-deploy-blue
spec:
  replicas: 1
  selector:
    matchLabels:
      run: nginx-blue
  template:
    metadata:
      labels:
        run: nginx-blue
    spec:
      volumes:
      - name: webdata
        emptyDir: {}
      initContainers:
      - name: web-content
        image: busybox
        volumeMounts:
        - name: webdata
          mountPath: "/webdata"
        command: ["/bin/sh", "-c", 'echo "<h1>I am <font color=blue>BLUE</font></h1>" > /webdata/index.html']
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: webdata
          mountPath: "/usr/share/nginx/html"
------------------------------------------------

--> kubectl apply -f test1-nginx.yaml
--> kubectl apply -f test1-green.yaml
--> kubectl apply -f test1-blue.yaml

After check
--> kubectl get deployment
pods will created automatically when deployment is created
--> kubectl get pod

Eg:
> kubectl get deployment
nginx-deploy-blue                        1/1     1            1           18h
nginx-deploy-green                       1/1     1            1           18h
nginx-deploy-main                        1/1     1            1           18h
++++++

> kubectl get pods 
nginx-deploy-blue-9784c656c-2tthz        1/1     Running     192.168.130.129   e2e-70-246   
nginx-deploy-green-786b88cb6-6rz5        1/1     Running     192.168.132.65    e2e-71-168   
nginx-deploy-main-5844dccccb-22pc9       1/1     Running     192.168.188.3     e2e-71-104  
-------------------------------------------------

Next step is to expose deployment to create svc
--> kubectl expose deploy nginx-deploy-main --type=ClusterIP
--> kubectl expose deploy nginx-deploy-blue --type=ClusterIP
--> kubectl expose deploy nginx-deploy-green --type=ClusterIP

Eg:
> kubectl get svc
nginx-deploy-blue                        ClusterIP      10.107.71.246    <none>           80/TCP          
nginx-deploy-green                       ClusterIP      10.107.121.67    <none>           80/TCP             
nginx-deploy-main                        ClusterIP      10.96.110.88     <none>           80/TCP        
kubernetes                               ClusterIP      10.96.0.1        <none>           443/TCP                      
my-nginx-nginx-ingress-controller        LoadBalancer   10.102.35.154    164.52.202.169   80:30460/TCP,443:30437/TCP   
my-nginx-nginx-ingress-default-backend   ClusterIP      10.102.196.175   <none>           80/TCP   
-------------------------------------------------

Next step going create ingress.yaml file

root@e2e-71-102:~# cat test2-ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /registerRequest
  name: ingress-test2
spec:
  rules:
  - host: livfit05.providerdom.com 
    http:
      paths:
      - path: /
        backend:
          serviceName: nginx-deploy-main
          servicePort: 80      
      - path: /green
        backend:
          serviceName: nginx-deploy-green
          servicePort: 80    
      - path: /blue
        backend:
          serviceName: nginx-deploy-blue 
          servicePort: 80            

--> kubectl apply -f test2-ingress.yaml
-------------------------------------------------
output:

http://livfit05.providerdom.com --> it should show nginx welcome page (Success)
http://livfit05.providerdom.com/green --> it shows green text (Success)
http://livfit05.providerdom.com/blue --> it show blue text (Success)

As of now cluster setup is working as expected.
=================================================================================
======******************************************************************=========
=================================================================================





3. Jenkins configuration and testing



3.1.	 jenkins setup.
------------------

Refer jenkins setup.docx
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------


3.2.	 script creation for each services.
-------------------------------------------
create a new branch name Deploy for all services in git lab

Eg:
Devops/livfit-devops-script/jenkinsfile: ci-registerRequest.jenkinsfile

If we are going to write script for registerRequest service 

Step1: check pom.xml in a code and verify what are all dependencies required to run registerRequest services.
Step2: 
For registerRequest --> we need 3 common dependencies  
They are: eureka-coommon, keycloakservice, livfit-communication 
Step3: next registerRequest service

________________________________________________
Eg script:

 pipeline {
 agent any
 stages {
     stage('CodeBuildForEurekaCommon') {
       steps {
         cleanWs()
         git url: 'git@gitlab.com:livfit-microservice/livfit-eureka-common.git',
         credentialsId: 'tomcat',
         branch: 'Deploy'
         sh "mvn clean install"
         sh ''' ls -ltr target/'''
    }
}
     stage('CodeBuildForKeycloak') {
       steps {
         git url: 'git@gitlab.com:livfit-microservice/keycloakservice.git',
         credentialsId: 'tomcat',
         branch: 'Deploy'
         sh "mvn clean install"
         sh ''' ls -ltr target/'''
    }
}
     stage('CodeBuildForLivfitCommunication') {
       steps {
         git url: 'git@gitlab.com:zmast.devteam2/livfit-communication.git',
         credentialsId: 'tomcat',
         branch: 'Deploy'
         sh "mvn clean install"
         sh ''' ls -ltr target/'''
    }
}
     stage('CodeBuildForLivfitREG') {
       steps {
         git url: 'git@gitlab.com:livfit-microservice/livfit-registration-request-service.git',
         credentialsId: 'tomcat',
         branch: 'Deploy'
         sh "mvn clean install"
         sh ''' ls -ltr target/'''
       }
     }
    stage('deployTok8s') {
       steps {
         git url: 'git@gitlab.com:zmast.devteam2/livfit-devops-script.git',
         credentialsId: 'tomcat',
         branch: 'master',
         changelog: false
          }
     }
     stage(BuildDockerImage) {
       steps {
           withEnv(["micro_service_name=livfit-registration-request-service", "port=8210"]) {
           sh ''' echo "Dockerfile for the livfit microservice"
                  
                  echo 'FROM openjdk:8-alpine
                  EXPOSE '$port'
                  ADD target/*.jar '${micro_service_name}'.jar
                  ENTRYPOINT ["java","-jar","/'${micro_service_name}'.jar"]' > Dockerfile
                  
                  echo "Building docker image for livfit microservice"
                  image=`docker build . | grep built | awk '{ print $3}'`
                  
                  docker tag $image 172.16.120.77:5000/${micro_service_name}:latest

                  docker push 172.16.120.77:5000/${micro_service_name}:latest
                  
                  docker images

                  helm_check=`helm list | grep ${micro_service_name} | wc -l`
                  
                  if [ $helm_check = 1 ]; then
                      helm upgrade ${micro_service_name} ./helm/livfit --debug --set app.ms_name=${micro_service_name} --set service.port=${port} --set service.path=registerRequest
                  else
                      helm install ${micro_service_name} ./helm/livfit --debug --set app.ms_name=${micro_service_name} --set service.port=${port} --set service.path=registerRequest
                  fi

                  helm list''' 
           }
       }
     }
}
post {
	success {
	  echo 'Job is success, Sending mail to Developers'
      mail bcc: '', body: "<b>${env.JOB_NAME} Build is triggered for your latest commit, changes is deployed on kubernetes cluster!!</b><br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> URL de build: ${env.BUILD_URL}", cc: '', charset: 'UTF-8', from: '', mimeType: 'text/html', replyTo: '', subject: "CI CD job: ${env.JOB_NAME}", to: "jenkinslog@myzmast.com"
  	}
	failure {
	  echo 'Job is failed, Sending mail to Developers'
      mail bcc: '', body: "<b>${env.JOB_NAME} Build is triggered for your latest commit,build job is failed!!. Please retrigger the job</b><br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> URL de build: ${env.BUILD_URL}", cc: '', charset: 'UTF-8', from: '', mimeType: 'text/html', replyTo: '', subject: "CI CD job: ${env.JOB_NAME}", to: "jenkinslog@myzmast.com"
  	}
  }
}
_____________________________________________________________

for every services we suppose to create a script like this
similarly we have to do for ci-jobs

Devops/livfit-devops-script/CI-jobs: livfitregisterRequest.groovy

________________________________________________
Eg script:

pipelineJob("CI-Livfit-services/CI-livfit-registration-request-service") {
      triggers {
        gitlabPush {
            buildOnMergeRequestEvents(false)
            buildOnPushEvents(true)
            enableCiSkip(false)
            setBuildDescription(false)
            rebuildOpenMergeRequest('never')
            includeBranches('Deploy')
        }
            gitlab {
               secretToken('263f9c6c68e4368bb151ab45500c52d7')
        }
      }
  definition {
    cps {
      script(readFileFromWorkspace('jenkinsfile/CI-livfit-registration-request-service.jenkinsfile'))
	  sandbox()
    }
  }
}
________________________________________________

--> after modifing of this script push the devops script to livfit-devops project in zmast.deavteam2 gitlab account

http://101.53.157.158/jenkins/job/CI-Livfit-services/
go to jenkins page: 
-> seedjob-pipeline -> build now
-> CI-livfit-services -> buildnow 
for 1st time we suppose to do this in jenkins after that it will trigger automatically.
 
Like this we have to perform for all the remaining services.
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------



3.3.	 Ci/cd pipeline (gitlab & jenkins configuration)
--------------------------------------------------------
--> login zmast.devteam2 in gitlab 
--> livfit-organization-service/setting/webhooks
--> simutaneously

--> go to jenkins page
--> ci-livfit-services/ ci-livfit-organizatiion-service/configure
--> build trigger copy the url
eg: http://101.53.157.158/jenkins/project/CI-Livfit-services/CI-livfit-organization-service

--> go back to gitlab webooks
--> copy the url in webhook
--> token is same for all services 263f9c6c68e4368bb151ab45500c52d7 paste this in secret token
--> Trigger/ check the push events type the branch name as Deploy & uncheck the Enable SSL verification and Add webhook.
---------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------

After this when ever the code is pushed to Deploy the updated code will automatically changed in jenkins job also. 
